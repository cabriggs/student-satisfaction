## Files included

student-satisfaction.ipynb: a Jupyter notebook containing the analysis and narrative.
student-satisfaction.pdf: a summary of the research question and conclusions
evals\_manual\_rated.csv: the data to be analyzed, including the text comments manually rated for positivity/negativity
evals\_data\_dict.txt: a data dictionary
evals\_with\_manual\_and\_auto\_sent.csv: the same data set, but with automatic determination of text positivity/negativity included

## Background

Opinionnaires were administered to students in courses taught by the analysis author between August 2015 and May 2018. These opinionnaires included questions about specific aspects of the instructor and course, such as textbook quality and instructor preparedness, as well as an "overall" instructor quality rating. This study takes the latter as the target variable, and the rest of the questions as features.

## Problem statement

Which specific aspects of course and instructor quality are most important to the average student's overall rating of a course/instructor? Which factors matter less?

## What data

Please see the data dictionary.

## What methods

Automated text sentimentality processing is applied to the text field responses. This is matched against manual ratings of the same.

Correlation analysis examines the relevance of each aspect of course quality individually to the target variable. This would answer the research question if it can be assumed that each of these qualities has an independent impact on overall course quality perception. To test the last, models are trained on the data to determine whether the target variable is better predicted by some nonlinear combination of the features.

### Technology

The analysis was performed in a Jupyter notebook, leveraging the textblob library for text sentiment analysis and sklearn for machine learning.

## What conclusions

An instructor who is teaching under conditions similar to those in which the data was gathered can exert some influence on student opinions of instruction quality. Students rate an instructor higher when s/he is perceived to teach effectively and examine fairly. These findings are not surprising. What is revealed by the analysis which is not intuitive: the categories of performance which are most important to student opinion of instruction quality are, in decreasing order, Question 7, Question 8 ("Examinations/assignments reflect materials covered in the class."), Question 12 ("My instructor responds to student Questions and concerns in a constructive manner."), and Question 13 ("My instructor speaks clearly and is easily understood."). Preparedness for class and enthusiasm are comparatively less important, and student opinion of required course material is barely relevant.

A product of the analysis is a pair of models each with good prediction accuracy of overall student opinion of instruction quality. Potential applications are: instructors may use this algorithm to decide how shifting efforts between categories of instructional objectives may affect net ratings; administrators at similar institutions may apply the models to ratings data of job applicants to estimate work performance if hired.

## What questions remain

As always, a model no longer applies when new data is generated by participants bearing the model in mind. To be concrete: if an instructor knows that the perceived relevance of examinations to course material weighs highly in student evaluations, and goes on to purposefully make the material as obviously relevant as possible, will this still be as important factor to the students' ratings of instructor quality? Will the weighting change? Through many iterations of data generation and modeling, the process would presumably converge.
